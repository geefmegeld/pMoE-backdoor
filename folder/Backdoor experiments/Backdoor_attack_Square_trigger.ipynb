{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qMkNHM0g5Qke"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import copy\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from random import random\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import traceback\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ycU9O-5Qkg"
      },
      "source": [
        "# Gating Routers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cHgmRT3H5Qkg"
      },
      "outputs": [],
      "source": [
        "class gate(tf.keras.layers.Layer):\n",
        "    def __init__(self, k, gating_kernel_size, strides=(1,1), padding = 'valid',\n",
        "                 data_format = 'channels_last', gating_activation = None,\n",
        "                 gating_kernel_initializer = tf.keras.initializers.RandomNormal, **kwargs):\n",
        "\n",
        "        super(gate, self).__init__(**kwargs)\n",
        "        self.k = k\n",
        "        self.gating_kernel_size = gating_kernel_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.data_format = data_format\n",
        "        self.gating_activation = tf.keras.activations.get(gating_activation)\n",
        "        self.gating_kernel_initializer = gating_kernel_initializer\n",
        "        self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
        "\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        gating_kernel_shape = self.gating_kernel_size + (input_dim, 1)\n",
        "        self.gating_kernel = self.add_weight(shape=gating_kernel_shape,\n",
        "                                      initializer=self.gating_kernel_initializer,\n",
        "                                      name='gating_kernel')\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        gating_outputs = tf.keras.backend.conv2d(inputs, self.gating_kernel, strides=self.strides,\n",
        "                                  padding=self.padding,data_format=self.data_format)\n",
        "\n",
        "        gating_outputs = tf.transpose(gating_outputs, perm=(0,3,1,2))\n",
        "        x = tf.shape(gating_outputs)[2]\n",
        "        y = tf.shape(gating_outputs)[3]\n",
        "        gating_outputs = tf.reshape(gating_outputs,(tf.shape(gating_outputs)[0],tf.shape(gating_outputs)[1],\n",
        "                                                    x*y))\n",
        "\n",
        "        gating_outputs = self.gating_activation(gating_outputs)\n",
        "        # print(\"gating output: \", gating_outputs.shape)\n",
        "        [values, indices] = tf.math.top_k(gating_outputs,k=self.k, sorted=False)\n",
        "        # print(\"value output: \", values.shape)\n",
        "        # print(\"indice before output: \", indices.shape)\n",
        "        indices = tf.reshape(indices,(tf.shape(indices)[0]*tf.shape(indices)[1],tf.shape(indices)[2]))\n",
        "        # print(\"indice after output: \", indices.shape)\n",
        "        values = tf.reshape(values, (tf.shape(values)[0]*tf.shape(values)[1], tf.shape(values)[2]))\n",
        "        batch_t, k_t = tf.unstack(tf.shape(indices), num=2)\n",
        "\n",
        "        n=tf.shape(gating_outputs)[2]\n",
        "\n",
        "        indices_flat = tf.reshape(indices, [-1]) + tf.math.floordiv(tf.range(batch_t * k_t), k_t) * n\n",
        "        ret_flat = tf.math.unsorted_segment_sum(tf.reshape(values, [-1]), indices_flat, batch_t * n)\n",
        "        ret_rsh=tf.reshape(ret_flat, [batch_t, n])\n",
        "        ret_rsh_3=tf.reshape(ret_rsh,(tf.shape(gating_outputs)[0],tf.shape(gating_outputs)[1],tf.shape(gating_outputs)[2]))\n",
        "\n",
        "        new_gating_outputs = tf.reshape(ret_rsh_3,(tf.shape(ret_rsh_3)[0],tf.shape(ret_rsh_3)[1],x,y))\n",
        "        new_gating_outputs = tf.transpose(new_gating_outputs, perm=(0,2,3,1))\n",
        "        new_gating_outputs = tf.repeat(new_gating_outputs,tf.shape(self.gating_kernel)[0]*tf.shape(self.gating_kernel)[1]*tf.shape(self.gating_kernel)[2],axis=3)\n",
        "        new_gating_outputs=tf.reshape(new_gating_outputs,(tf.shape(new_gating_outputs)[0],tf.shape(new_gating_outputs)[1],tf.shape(new_gating_outputs)[2],tf.shape(self.gating_kernel)[0],tf.shape(self.gating_kernel)[1],tf.shape(self.gating_kernel)[2]))\n",
        "        new_gating_outputs=tf.transpose(new_gating_outputs,perm=(0,1,3,2,4,5))\n",
        "        new_gating_outputs=tf.reshape(new_gating_outputs,(tf.shape(new_gating_outputs)[0],tf.shape(new_gating_outputs)[1]*tf.shape(new_gating_outputs)[2],tf.shape(new_gating_outputs)[3]*tf.shape(new_gating_outputs)[4],tf.shape(new_gating_outputs)[5]))\n",
        "        outputs = inputs*new_gating_outputs\n",
        "        return outputs, indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SERp0GMl5Qkh"
      },
      "source": [
        "# Wideresnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qMoWs8YN5Qkh"
      },
      "outputs": [],
      "source": [
        "initializer_gate=keras.initializers.RandomNormal(mean=0.0,stddev=0.0001)\n",
        "\n",
        "def WideResnetBlock(x, channels, strides, channel_mismatch=False):\n",
        "\n",
        "    identity = x\n",
        "\n",
        "    out = layers.BatchNormalization()(x)\n",
        "    out = layers.ReLU()(out)\n",
        "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=strides, padding='same')(out)\n",
        "\n",
        "    out = layers.BatchNormalization()(out)\n",
        "    out = layers.ReLU()(out)\n",
        "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=1, padding='same')(out)\n",
        "\n",
        "    if channel_mismatch is not False:\n",
        "        identity = layers.Conv2D(filters=channels, kernel_size=1, strides=strides, padding='valid')(identity)\n",
        "\n",
        "    out = layers.Add()([identity, out])\n",
        "\n",
        "    return out\n",
        "\n",
        "def WideResnetGroup(x, num_blocks, channels, strides):\n",
        "\n",
        "    x = WideResnetBlock(x=x, channels=channels, strides=strides, channel_mismatch=True)\n",
        "\n",
        "    for _ in range(num_blocks - 1):\n",
        "        x = WideResnetBlock(x=x, channels=channels, strides=(1, 1))\n",
        "\n",
        "    return x\n",
        "\n",
        "def WideResnet(x, num_blocks, k, num_classes=10):\n",
        "    widths = [int(v * k) for v in (16, 32, 64)]\n",
        "\n",
        "    x = layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same')(x)\n",
        "    x = WideResnetGroup(x, num_blocks, widths[0], strides=(1, 1))\n",
        "    x = WideResnetGroup(x, num_blocks, widths[1], strides=(2, 2))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(filters=640, kernel_size=3, strides=2, padding='same')(x)\n",
        "\n",
        "    x_1, indices_1 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
        "    x_2, indices_2 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
        "    x_3, indices_3 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
        "    x_4, indices_4 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
        "\n",
        "    x_1 = layers.BatchNormalization()(x_1)\n",
        "    x_2 = layers.BatchNormalization()(x_2)\n",
        "    x_3 = layers.BatchNormalization()(x_3)\n",
        "    x_4 = layers.BatchNormalization()(x_4)\n",
        "\n",
        "    x_1 = layers.ReLU()(x_1)\n",
        "    x_2 = layers.ReLU()(x_2)\n",
        "    x_3 = layers.ReLU()(x_3)\n",
        "    x_4 = layers.ReLU()(x_4)\n",
        "\n",
        "    x_1 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_1)\n",
        "    x_2 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_2)\n",
        "    x_3 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_3)\n",
        "    x_4 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_4)\n",
        "\n",
        "    x = tf.keras.layers.concatenate([x_1, x_2, x_3, x_4])\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.AveragePooling2D((8,8))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(units=num_classes, activation='softmax')(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4N0uBR45Qkh"
      },
      "source": [
        "# Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9JzZyLXuKtoQ"
      },
      "outputs": [],
      "source": [
        "def visualize_expert_specialization(grid_tracking):\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    for expert_id, grid in grid_tracking.items():\n",
        "        ax = axes[expert_id - 1]\n",
        "        cax = ax.imshow(grid, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "        ax.set_title(f\"Expert {expert_id} Specialization\")\n",
        "        ax.set_xlabel(\"Columns\")\n",
        "        ax.set_ylabel(\"Rows\")\n",
        "        fig.colorbar(cax, ax=ax)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SdPBT8XGUb4A"
      },
      "outputs": [],
      "source": [
        "def visualize_expert_specialization_by_epoch(patch_assignments):\n",
        "    for epoch, epoch_assignments in enumerate(patch_assignments, start=1):\n",
        "        # Create grid tracking for this epoch\n",
        "        grid_tracking = {expert_id: np.zeros((8, 8), dtype=int) for expert_id in range(1, 5)}\n",
        "        for assignment in epoch_assignments:\n",
        "            expert_id = assignment[\"expert\"]\n",
        "            for (row, col) in assignment[\"grid_coordinates\"]:\n",
        "                grid_tracking[expert_id][row, col] += 1\n",
        "\n",
        "        # Plot for this epoch\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        for expert_id, grid in grid_tracking.items():\n",
        "            ax = axes[expert_id - 1]\n",
        "            cax = ax.imshow(grid, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "            ax.set_title(f\"Expert {expert_id} Specialization (Epoch {epoch})\")\n",
        "            ax.set_xlabel(\"Columns\")\n",
        "            ax.set_ylabel(\"Rows\")\n",
        "            fig.colorbar(cax, ax=ax)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VeQ--CCi6g9c"
      },
      "outputs": [],
      "source": [
        "def compute_patch_sums(images, patch_size=(8, 8)):\n",
        "    num_images, img_height, img_width, _ = images.shape\n",
        "    patch_sums = np.zeros((num_images, patch_size[0], patch_size[1]))\n",
        "\n",
        "    for i in range(patch_size[0]):  # Divide into rows\n",
        "        for j in range(patch_size[1]):  # Divide into columns\n",
        "            patch_sums[:, i, j] = np.sum(\n",
        "                images[:,\n",
        "                       i * (img_height // patch_size[0]): (i + 1) * (img_height // patch_size[0]),\n",
        "                       j * (img_width // patch_size[1]): (j + 1) * (img_width // patch_size[1]),\n",
        "                       :],\n",
        "                axis=(1, 2, 3)\n",
        "            )\n",
        "    return patch_sums"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-d9cWBl5Qki"
      },
      "source": [
        "# Trigger generation methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3ySd1qQa5Qki"
      },
      "outputs": [],
      "source": [
        "class GenerateSQRTrigger:\n",
        "    \"\"\"\n",
        "    A class that creates a random square pattern that is used as a trigger for an\n",
        "    image dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, pos_label, dataset='mnist'):\n",
        "\n",
        "        datasets_dimensions = {\"mnist\": (28, 28, 1),\n",
        "                               \"cifar10\": (32, 32, 3),\n",
        "                               \"fmnist\": (28, 28, 1)}\n",
        "\n",
        "        dims = datasets_dimensions[dataset]\n",
        "\n",
        "        if size[0] != size[1]:\n",
        "            raise Exception(\"The size of the trigger must be square.\")\n",
        "\n",
        "        if pos_label.lower() not in [\"upper-left\", \"upper-mid\", \"upper-right\", \"mid-left\", \"mid-mid\", \"mid-right\",\n",
        "                                     \"lower-left\",\n",
        "                                     \"lower-mid\", \"lower-right\"]:\n",
        "            raise Exception(\n",
        "                \"The position of the trigger must be one of the following: upper-left, upper-mid, upper-right, mid-left, mid-mid, mid-right, lower-left, lower-mid, lower-right\")\n",
        "\n",
        "        if size[0] > dims[0] or size[1] > dims[1]:\n",
        "            raise Exception(\"The size of the trigger is too large for the dataset items.\")\n",
        "\n",
        "        self.dims = dims\n",
        "        self.size = size\n",
        "        self.pos_label = pos_label\n",
        "        # pos == position; coordinates\n",
        "        self.pos_coords = self._gen_pos_square()\n",
        "\n",
        "        trigger = np.zeros(self.dims, dtype=np.float32)\n",
        "        self.crafted_trigger = self.create_trigger_square(trigger)\n",
        "\n",
        "    def _gen_pos_square(self):\n",
        "        if self.pos_label == \"upper-left\":\n",
        "            return (0, 0)\n",
        "        elif self.pos_label == \"upper-mid\":\n",
        "            return (0, self.dims[1] // 2 - self.size[1] // 2)\n",
        "        elif self.pos_label == \"upper-right\":\n",
        "            return (0, self.dims[1] - self.size[1])\n",
        "\n",
        "        elif self.pos_label == \"mid-left\":\n",
        "            return (self.dims[0] // 2 - self.size[0] // 2, 0)\n",
        "        elif self.pos_label == \"mid-mid\":\n",
        "            return (self.dims[0] // 2 - self.size[0] // 2,\n",
        "                    self.dims[1] // 2 - self.size[1] // 2)\n",
        "        elif self.pos_label == \"mid-right\":\n",
        "            return (self.dims[0] // 2 - self.size[0] // 2, self.dims[1] - self.size[1])\n",
        "\n",
        "        elif self.pos_label == \"lower-left\":\n",
        "            return (self.dims[0] - self.size[0], 0)\n",
        "        elif self.pos_label == \"lower-mid\":\n",
        "            return (self.dims[0] - self.size[0], self.dims[1] // 2 - self.size[1] // 2)\n",
        "        elif self.pos_label == \"lower-right\":\n",
        "            return (self.dims[0] - self.size[0], self.dims[1] - self.size[1])\n",
        "\n",
        "    def create_trigger_square(self, trigger):\n",
        "        \"\"\"Create a square trigger.\"\"\"\n",
        "        base_x, base_y = self.pos_coords\n",
        "        for x in range(self.size[0]):\n",
        "            for y in range(self.size[1]):\n",
        "                trigger[base_x + x][base_y + y] = \\\n",
        "                    np.ones((self.dims[2]))\n",
        "\n",
        "        return trigger\n",
        "\n",
        "    def apply_trigger(self, img):\n",
        "        \"\"\"applies the trigger on the image.\"\"\"\n",
        "\n",
        "        base_x, base_y = self.pos_coords\n",
        "        for x in range(self.size[0]):\n",
        "            for y in range(self.size[1]):\n",
        "                img[base_x + x][base_y + y] = self.crafted_trigger[base_x + x][base_y + y]\n",
        "        return img\n",
        "\n",
        "class GenerateBlendedTrigger:\n",
        "    \"\"\"\n",
        "    A class that uses images of the same dimensions as the dataset as triggers\n",
        "    that will be blended with the clean images.\n",
        "\n",
        "    We will use a random pattern or a hello-kitty image as the original paper\n",
        "    (https://arxiv.org/pdf/1712.05526.pdf).\n",
        "    \"\"\"\n",
        "    hello_kitty_path = \"./hello_kitty.jpg\"\n",
        "\n",
        "    def __init__(self, dataset, trigger):\n",
        "\n",
        "        datasets_dimensions = {\"mnist\": (28, 28, 1),\n",
        "                               \"cifar10\": (32, 32, 3),\n",
        "                               \"fmnist\": (28, 28, 1)}\n",
        "\n",
        "        # Use a hardcoded seed for reproducibility\n",
        "        dims = datasets_dimensions[dataset]\n",
        "\n",
        "        if trigger not in [\"random\", \"hello-kitty\"]:\n",
        "            raise Exception(f\"Pick 'random' or 'hello-kitty' trigger\")\n",
        "\n",
        "        if dataset not in datasets_dimensions:\n",
        "            raise Exception(f\"Dataset is not supported\")\n",
        "\n",
        "        self.dims = dims\n",
        "        self.dataset = dataset\n",
        "\n",
        "        # Generate the correct trigger\n",
        "        self.crafted_trigger = self.trigger_blended(trigger)\n",
        "\n",
        "    def trigger_blended(self, trigger):\n",
        "        \"\"\"Prepare the trigger for blended attack.\"\"\"\n",
        "        if trigger == \"hello-kitty\":\n",
        "            # Load kitty\n",
        "            img = Image.open(self.hello_kitty_path)\n",
        "\n",
        "            # Resize to dimensions\n",
        "            tmp = img.resize(self.dims[:-1])\n",
        "\n",
        "            if self.dims[2] == 1:\n",
        "                tmp = ImageOps.grayscale(tmp)\n",
        "\n",
        "            tmp = np.asarray(tmp)\n",
        "            # This is needed in case the image is grayscale (width x height) to\n",
        "            # add the channel dimension\n",
        "            tmp = tmp.reshape((self.dims))\n",
        "            trigger_array = tmp / 255\n",
        "        else:\n",
        "            # Create a np.array with the correct dimensions\n",
        "            # fill the pixels with random values\n",
        "            trigger_array = (np.random.random((self.dims)))\n",
        "\n",
        "        return trigger_array\n",
        "\n",
        "    def apply_trigger(self, img):\n",
        "        \"\"\"applies the trigger on the image.\"\"\"\n",
        "        crafted_trigger_normalized = self.crafted_trigger\n",
        "        if crafted_trigger_normalized.max() > 1:\n",
        "            crafted_trigger_normalized = crafted_trigger_normalized / 255.0\n",
        "\n",
        "        # Ensure the input image is normalized to [0, 1]\n",
        "        if img.max() > 1:\n",
        "            img = img / 255.0\n",
        "\n",
        "        # Blend the images\n",
        "        img = (img + crafted_trigger_normalized) / 2\n",
        "        # plt.figure(figsize=(4, 4))\n",
        "        # plt.imshow(img, cmap='gray' if self.dims[-1] == 1 else None)\n",
        "        # plt.axis('off')\n",
        "        # plt.title(\"Crafted Trigger\")\n",
        "        # plt.show()\n",
        "        # print(f\"Blended image min: {img.min()}, max: {img.max()}\")\n",
        "        return img.astype(np.float32)\n",
        "\n",
        "class GenerateWarpedTrigger:\n",
        "    \"\"\"\n",
        "    A class that generates a warped trigger using a distortion grid for backdoor attacks.\n",
        "    Compatible with TensorFlow.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, s=0.25, grid_rescale=1.0, k = 2, input_height = 32):\n",
        "        \"\"\"\n",
        "        Initialize the warped trigger generator.\n",
        "        :param dataset: Dataset name (e.g., 'mnist', 'cifar10', etc.) for defining image dimensions.\n",
        "        :param s: Strength of the warping effect.\n",
        "        :param grid_rescale: Rescaling factor for the distortion grid.\n",
        "        \"\"\"\n",
        "        datasets_dimensions = {\"mnist\": (28, 28, 1),\n",
        "                               \"cifar10\": (32, 32, 3),\n",
        "                               \"fmnist\": (28, 28, 1)}\n",
        "\n",
        "        if dataset not in datasets_dimensions:\n",
        "            raise Exception(f\"Dataset is not supported\")\n",
        "\n",
        "        self.dims = datasets_dimensions[dataset]\n",
        "        self.s = s\n",
        "        self.k = k\n",
        "        self.input_height = input_height\n",
        "        self.grid_rescale = grid_rescale\n",
        "\n",
        "        # Initialize the identity grid and noise grid for warping\n",
        "        self.identity_grid, self.noise_grid = self.generate_main_grid()\n",
        "\n",
        "    def generate_main_grid(self):\n",
        "        \"\"\"\n",
        "        Generate the identity and noise grids for the warped trigger.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create coarse random noise grid\n",
        "        grid_noise = tf.random.uniform(\n",
        "            shape=(1, self.k, self.k, 2), minval=-1.0, maxval=1.0\n",
        "        )\n",
        "        grid_noise = grid_noise / tf.reduce_mean(tf.abs(grid_noise))\n",
        "\n",
        "        # Upsample the coarse noise to match the input height and width\n",
        "        noise_grid = tf.image.resize(grid_noise, size=(self.input_height, self.input_height), method=\"bicubic\")\n",
        "        noise_grid = tf.clip_by_value(noise_grid, -1.0, 1.0)  # Clamp values for stability\n",
        "\n",
        "        # Create the identity grid\n",
        "        array1d = tf.linspace(-1.0, 1.0, self.input_height)\n",
        "        x, y = tf.meshgrid(array1d, array1d)\n",
        "        identity_grid = tf.stack([y, x], axis=-1)\n",
        "        identity_grid = identity_grid[tf.newaxis, ...]  # Add batch dimension\n",
        "\n",
        "        return identity_grid, noise_grid\n",
        "\n",
        "    def _grid_sample(self, image, grid):\n",
        "        \"\"\"\n",
        "        TensorFlow implementation of grid sampling for image warping.\n",
        "        :param image: The input image tensor with shape (batch_size, height, width, channels).\n",
        "        :param grid: The grid tensor with shape (batch_size, height, width, 2).\n",
        "        :return: Warped image tensor.\n",
        "        \"\"\"\n",
        "        batch_size, height, width, channels = image.shape\n",
        "\n",
        "        # Split grid into x and y components\n",
        "        grid_y, grid_x = tf.split(grid, 2, axis=-1)\n",
        "\n",
        "        # Rescale normalized grid coordinates to image pixel indices\n",
        "        grid_x = tf.cast((grid_x + 1.0) * 0.5 * tf.cast(width - 1, tf.float32), tf.int32)\n",
        "        grid_y = tf.cast((grid_y + 1.0) * 0.5 * tf.cast(height - 1, tf.float32), tf.int32)\n",
        "\n",
        "        # Remove the last dimension of grid_x and grid_y to match batch_indices shape\n",
        "        grid_x = tf.squeeze(grid_x, axis=-1)  # Shape: (batch_size, height, width)\n",
        "        grid_y = tf.squeeze(grid_y, axis=-1)  # Shape: (batch_size, height, width)\n",
        "\n",
        "        # Create batch indices for gather_nd\n",
        "        batch_indices = tf.range(batch_size)[:, tf.newaxis, tf.newaxis]  # Shape: (batch_size, 1, 1)\n",
        "        batch_indices = tf.tile(batch_indices, [1, height, width])  # Shape: (batch_size, height, width)\n",
        "\n",
        "        # Clip grid indices to stay within image bounds\n",
        "        grid_x = tf.clip_by_value(grid_x, 0, width - 1)\n",
        "        grid_y = tf.clip_by_value(grid_y, 0, height - 1)\n",
        "\n",
        "        # Stack indices for gather_nd\n",
        "        indices = tf.stack([batch_indices, grid_y, grid_x], axis=-1)\n",
        "\n",
        "        sampled_image = tf.gather_nd(image, indices)\n",
        "\n",
        "        return sampled_image\n",
        "\n",
        "    def poison(self, image):\n",
        "        \"\"\"\n",
        "        Apply a warping trigger to the image.\n",
        "        :param image: A NumPy array representing the input image.\n",
        "        :return: A NumPy array of the warped image.\n",
        "        \"\"\"\n",
        "        # Ensure the input image is normalized\n",
        "        if image.max() > 1.0:\n",
        "            image = image / 255.0\n",
        "\n",
        "        # Expand dimensions to (batch_size, height, width, channels)\n",
        "        image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
        "        if len(image_tensor.shape) == 3:  # Add batch dimension if missing\n",
        "            image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
        "\n",
        "        # Generate the warped grid\n",
        "        grid_temps = (self.identity_grid + self.s * self.noise_grid / 32) * self.grid_rescale\n",
        "        grid_temps = tf.clip_by_value(grid_temps, -1.0, 1.0)\n",
        "\n",
        "        # Warp the image using TensorFlow's grid_sample equivalent\n",
        "        poisoned_image = self._grid_sample(image_tensor, grid_temps)\n",
        "\n",
        "        # Squeeze batch dimension and convert back to NumPy\n",
        "        poisoned_image = tf.squeeze(poisoned_image, axis=0).numpy()\n",
        "\n",
        "        return poisoned_image\n",
        "\n",
        "\n",
        "    def apply_trigger(self, img):\n",
        "        \"\"\"\n",
        "        Alias for the poison function for consistency with other trigger generators.\n",
        "        :param img: Input image as a NumPy array.\n",
        "        :return: Warped image as a NumPy array.\n",
        "        \"\"\"\n",
        "        return self.poison(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating backdoor dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BBMzxM7Y5Qki"
      },
      "outputs": [],
      "source": [
        "class BackdoorDataset:\n",
        "    \"\"\"\n",
        "    TensorFlow-compatible dataset for backdoor attacks, enabling poisoning of specific samples.\n",
        "    \"\"\"\n",
        "    def __init__(self, clean_data, clean_labels, trigger_obj, epsilon=0.08,\n",
        "                 target_label=None, source_label=None, train=True, cifar=True):\n",
        "        \"\"\"\n",
        "        Initialize the backdoor dataset.\n",
        "        :param clean_data: Original dataset images (NumPy array).\n",
        "        :param clean_labels: Original dataset labels (one-hot encoded NumPy array).\n",
        "        :param trigger_obj: Instance of the GenerateSQRTrigger class.\n",
        "        :param epsilon: Fraction of samples to poison (default: 0.08 or 8%).\n",
        "        :param target_label: The target label for poisoned samples.\n",
        "        :param source_label: The source label for poisoned samples.\n",
        "        :param train: Whether this dataset is for training or testing.\n",
        "        \"\"\"\n",
        "        self.clean_data = clean_data\n",
        "        self.clean_labels = clean_labels\n",
        "        self.trigger_obj = trigger_obj\n",
        "        self.epsilon = epsilon\n",
        "        self.target_label = target_label\n",
        "        self.source_label = source_label\n",
        "        self.train = train\n",
        "        self.cifar = cifar\n",
        "\n",
        "        if train:\n",
        "            self.poisoned_data, self.poisoned_labels = self.get_train_set()\n",
        "        else:\n",
        "            self.poisoned_data, self.poisoned_labels = self.get_test_set()\n",
        "\n",
        "    def poison(self, img):\n",
        "        \"\"\"Poison an image by applying the trigger.\"\"\"\n",
        "        poisoned_img = self.trigger_obj.apply_trigger(img)\n",
        "        # plt.figure(figsize=(4, 4))\n",
        "        # plt.imshow(poisoned_img if self.clean_data.shape[-1] == 3 else np.squeeze(poisoned_img, -1),\n",
        "        #         cmap='gray' if self.clean_data.shape[-1] == 1 else None)\n",
        "        # plt.axis('off')\n",
        "        # plt.title(\"Poisoned Image\")\n",
        "        # plt.show()\n",
        "        return poisoned_img\n",
        "\n",
        "    def get_train_set(self):\n",
        "        \"\"\"Generate the poisoned training set.\"\"\"\n",
        "        poisoned_data = np.copy(self.clean_data)\n",
        "        if (isinstance(self.trigger_obj, GenerateBlendedTrigger) and self.trigger_obj.crafted_trigger is not None) or \\\n",
        "        isinstance(self.trigger_obj, GenerateWarpedTrigger):\n",
        "            poisoned_data = poisoned_data / 255  # Apply normalization\n",
        "\n",
        "        poisoned_labels = np.copy(self.clean_labels)\n",
        "\n",
        "        num_samples = self.clean_data.shape[0]\n",
        "        num_poisoned = int(self.epsilon * num_samples)\n",
        "        poisoned_indices = np.random.choice(num_samples, size=num_poisoned, replace=False)\n",
        "\n",
        "        for idx in poisoned_indices:\n",
        "            label_idx = np.argmax(self.clean_labels[idx])  # Convert one-hot label to scalar\n",
        "\n",
        "            if self.source_label is not None:\n",
        "                if label_idx == self.source_label:\n",
        "                    # Poison data and change the label to target label\n",
        "                    poisoned_data[idx] = self.poison(self.clean_data[idx])\n",
        "                    if self.cifar is True:\n",
        "                        poisoned_labels[idx] = tf.one_hot(self.target_label, depth=10).numpy()\n",
        "                    else:\n",
        "                        poisoned_labels[idx] = tf.one_hot(self.target_label, depth=43).numpy()\n",
        "                else:\n",
        "                    # Poison data but keep the original label\n",
        "                    poisoned_data[idx] = self.poison(self.clean_data[idx])\n",
        "                    # Label remains unchanged\n",
        "            else:\n",
        "                # Poison data and always change the label to target label\n",
        "                poisoned_data[idx] = self.poison(self.clean_data[idx])\n",
        "                if self.cifar is True:\n",
        "                    poisoned_labels[idx] = tf.one_hot(self.target_label, depth=10).numpy()\n",
        "                else:\n",
        "                    poisoned_labels[idx] = tf.one_hot(self.target_label, depth=43).numpy()\n",
        "\n",
        "        return poisoned_data, poisoned_labels\n",
        "\n",
        "    def get_test_set(self):\n",
        "        \"\"\"Generate the poisoned test set.\"\"\"\n",
        "        temp = deepcopy(self.clean_data)\n",
        "        poisoned_data = []\n",
        "        poisoned_labels = []\n",
        "\n",
        "        for idx in range(self.clean_data.shape[0]):\n",
        "            label_idx = np.argmax(self.clean_labels[idx])  # Convert one-hot label to scalar\n",
        "            if label_idx != self.target_label:\n",
        "                poisoned_data.append(self.poison(temp[idx]))\n",
        "                poisoned_labels.append(self.clean_labels[idx])\n",
        "        return np.array(poisoned_data), np.array(poisoned_labels)\n",
        "\n",
        "    def get_data(self):\n",
        "        \"\"\"Return the poisoned dataset.\"\"\"\n",
        "\n",
        "        return self.poisoned_data, self.poisoned_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attack evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ci5hdeSn5Qkj"
      },
      "outputs": [],
      "source": [
        "def find_source_indices(labels, source_label):\n",
        "    \"\"\"\n",
        "    Find indices of samples with the source label.\n",
        "    \"\"\"\n",
        "    indices = np.where(labels == source_label)[0]\n",
        "    return indices\n",
        "\n",
        "def find_non_source_indices(labels, source_label, target_label):\n",
        "    \"\"\"\n",
        "    Find indices of samples which do not have the source or target label.\n",
        "    \"\"\"\n",
        "    indices = np.where((labels != source_label) & (labels != target_label))[0]\n",
        "    return indices\n",
        "\n",
        "def count_non_source_misclassifications(original_labels, predicted_labels, source_label, target_label):\n",
        "    \"\"\"\n",
        "    Count misclassifications for non-source and non-target label samples.\n",
        "    \"\"\"\n",
        "    sub_non_source_total = 0\n",
        "    sub_misclassifications = 0\n",
        "\n",
        "    indices = find_non_source_indices(original_labels, source_label, target_label)\n",
        "    sub_non_source_total += len(indices)\n",
        "\n",
        "    for index in indices:\n",
        "        if predicted_labels[index] == target_label:\n",
        "            sub_misclassifications += 1\n",
        "    return sub_misclassifications, sub_non_source_total\n",
        "\n",
        "def count_source_specific_classifications(original_labels, predicted_labels, source_label, target_label):\n",
        "    \"\"\"\n",
        "    Count correct classifications for source label samples to target label.\n",
        "    \"\"\"\n",
        "    sub_total = 0\n",
        "    sub_correct = 0\n",
        "\n",
        "    indices = find_source_indices(original_labels, source_label)\n",
        "    sub_total += len(indices)\n",
        "\n",
        "    for index in indices:\n",
        "        if predicted_labels[index] == target_label:\n",
        "            sub_correct += 1\n",
        "    return sub_correct, sub_total\n",
        "\n",
        "def calculate_ASR(model, test_data, test_labels, target_label, source_label=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Calculate the Attack Success Rate (ASR) of the backdoored model.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    non_source_total = 0\n",
        "    misclassifications = 0\n",
        "\n",
        "    # Get model predictions\n",
        "    predictions = model.predict(test_data, batch_size=128)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    original_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "    if source_label is not None:\n",
        "        # Source-specific attack\n",
        "        sub_correct, sub_total = count_source_specific_classifications(original_labels, predicted_labels, source_label, target_label)\n",
        "        correct += sub_correct\n",
        "        total += sub_total\n",
        "\n",
        "        if verbose:\n",
        "            sub_misclassifications, sub_non_source_total = count_non_source_misclassifications(original_labels, predicted_labels, source_label, target_label)\n",
        "            misclassifications += sub_misclassifications\n",
        "            non_source_total += sub_non_source_total\n",
        "    else:\n",
        "        # Source-agnostic attack\n",
        "        for i in range(len(original_labels)):\n",
        "            if original_labels[i] != target_label:\n",
        "                total += 1\n",
        "                # print(\"original: \", original_labels[i], \"predict: \", predicted_labels[i])\n",
        "                if predicted_labels[i] == target_label:\n",
        "                    correct += 1\n",
        "\n",
        "\n",
        "    attack_acc = (correct * 100.0) / total\n",
        "    print(f\"Attack accuracy: {round(attack_acc, 2)}%\")\n",
        "\n",
        "    if source_label and verbose:\n",
        "        print(f\"Misclassifications: {misclassifications}\")\n",
        "        print(f\"Non-source total: {non_source_total}\")\n",
        "        misclassification_rate = (misclassifications * 100.0) / non_source_total\n",
        "        print(f\"False Positive Rate: {round(misclassification_rate, 2)}%\")\n",
        "\n",
        "    return attack_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FoMExvQ5Qkj"
      },
      "source": [
        "# Backdoor training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "79nwwVRG5Qkj"
      },
      "outputs": [],
      "source": [
        "trigger_generator_SQRT = GenerateSQRTrigger((4, 4), 'upper-left')\n",
        "trigger_generator_HK = GenerateBlendedTrigger(\"cifar10\", \"hello-kitty\")\n",
        "trigger_generator_Warped = GenerateWarpedTrigger(\"cifar10\")\n",
        "target_label = 0    # Target label for backdoor attack\n",
        "source_label = None    # Source label for backdoor attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Poisoning rate, can be adjusted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "poison_rate = 0.02  # 2% poisoning rate, adjust this to other poisoning rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CIFAR10 = True #If false, uses GTSRB dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dv3_kyYz5Qkj",
        "outputId": "a252bc27-cd79-4ca8-d368-b44b4c2b762a"
      },
      "outputs": [],
      "source": [
        "for s in [50000]:\n",
        "    # Loading the Data\n",
        "    if CIFAR10 is True:\n",
        "      print(\"CIFAR10 on\")\n",
        "      training_data_all = np.load('cifar_10_train_data_sorted.npy')\n",
        "      training_label_all = np.load('cifar_10_train_label_sorted.npy')\n",
        "      testing_data = np.load('cifar_10_test_data_sorted.npy')\n",
        "      testing_label = np.load('cifar_10_test_label_sorted.npy')\n",
        "\n",
        "      # Sampling training data\n",
        "      training_data = np.concatenate((training_data_all[0:0+(s//10)], training_data_all[5000:5000+(s//10)],\n",
        "                                      training_data_all[10000:10000+(s//10)], training_data_all[15000:15000+(s//10)],\n",
        "                                      training_data_all[20000:20000+(s//10)], training_data_all[25000:25000+(s//10)],\n",
        "                                      training_data_all[30000:30000+(s//10)], training_data_all[35000:35000+(s//10)],\n",
        "                                      training_data_all[40000:40000+(s//10)], training_data_all[45000:45000+(s//10)]), axis=0)\n",
        "      training_label = np.concatenate((training_label_all[0:0+(s//10)], training_label_all[5000:5000+(s//10)],\n",
        "                                        training_label_all[10000:10000+(s//10)], training_label_all[15000:15000+(s//10)],\n",
        "                                        training_label_all[20000:20000+(s//10)], training_label_all[25000:25000+(s//10)],\n",
        "                                        training_label_all[30000:30000+(s//10)], training_label_all[35000:35000+(s//10)],\n",
        "                                        training_label_all[40000:40000+(s//10)], training_label_all[45000:45000+(s//10)]), axis=0)\n",
        "\n",
        "      # training_data = poison_dataset(training_data, poison_rate, trigger_generator)\n",
        "      backdoor_training_dataset = BackdoorDataset(\n",
        "          clean_data=training_data,\n",
        "          clean_labels=tf.one_hot(training_label, depth=10).numpy(),\n",
        "          trigger_obj=trigger_generator_SQRT,\n",
        "          epsilon=poison_rate,\n",
        "          target_label=target_label,\n",
        "          source_label=source_label,\n",
        "          train=True\n",
        "      )\n",
        "      poisoned_training_data, poisoned_training_label = backdoor_training_dataset.get_data()\n",
        "\n",
        "      backdoor_test_dataset = BackdoorDataset(\n",
        "          clean_data=testing_data,\n",
        "          clean_labels=tf.one_hot(testing_label, depth=10).numpy(),\n",
        "          trigger_obj=trigger_generator_SQRT,  # Adjust this as needed for the trigger type\n",
        "          epsilon=poison_rate,  # Apply the same poisoning rate as training\n",
        "          target_label=target_label,\n",
        "          source_label=source_label,\n",
        "          train=False  # Specify that this is for testing\n",
        "      )\n",
        "      poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
        "      # 1-of-K encoding\n",
        "      training_label = tf.reshape(tf.one_hot(training_label, axis=1, depth=10, dtype=tf.float64), (s, 10)).numpy()\n",
        "      testing_label = tf.reshape(tf.one_hot(testing_label, axis=1, depth=10, dtype=tf.float64), (10000, 10)).numpy()\n",
        "\n",
        "      # Shuffling the training set\n",
        "      indices = tf.range(start=0, limit=tf.shape(training_data)[0], dtype=tf.int32)\n",
        "      shuffled_indices = tf.random.shuffle(indices)\n",
        "      training_data = tf.gather(training_data, shuffled_indices, axis=0)\n",
        "      training_label = tf.gather(training_label, shuffled_indices, axis=0)\n",
        "      poisoned_training_data = tf.gather(poisoned_training_data, shuffled_indices, axis=0)\n",
        "      poisoned_training_label = tf.gather(poisoned_training_label, shuffled_indices, axis=0)\n",
        "\n",
        "      # Normalizing and reshaping data\n",
        "      if isinstance(backdoor_training_dataset.trigger_obj, GenerateSQRTrigger):  # Check if the trigger is the square trigger\n",
        "          poisoned_training_data = poisoned_training_data / 255\n",
        "          poisoned_testing_data = poisoned_testing_data / 255\n",
        "\n",
        "      training_data=training_data/255\n",
        "      training_data=tf.cast(training_data,dtype=tf.dtypes.float32)\n",
        "      poisoned_training_data = tf.cast(poisoned_training_data, dtype=tf.dtypes.float32)\n",
        "      poisoned_testing_data = tf.cast(poisoned_testing_data, dtype=tf.dtypes.float32)\n",
        "\n",
        "\n",
        "      testing_data = testing_data / 255\n",
        "      testing_data = tf.cast(testing_data, dtype=tf.dtypes.float32)\n",
        "\n",
        "    else:\n",
        "        print(\"GTSRB as dataset\")\n",
        "        training_data = np.load('gtsrb_train_data_sorted.npy')\n",
        "        training_label = np.load('gtsrb_train_label_sorted.npy')\n",
        "        testing_data = np.load('gtsrb_test_data_sorted.npy')\n",
        "        testing_label = np.load('gtsrb_test_label_sorted.npy')\n",
        "\n",
        "        backdoor_training_dataset = BackdoorDataset(\n",
        "            clean_data=training_data,\n",
        "            clean_labels=tf.one_hot(training_label, depth=43).numpy(),\n",
        "            trigger_obj=trigger_generator_SQRT,\n",
        "            epsilon=poison_rate,\n",
        "            target_label=target_label,\n",
        "            source_label=source_label,\n",
        "            train=True,\n",
        "            cifar= False\n",
        "\n",
        "        )\n",
        "        poisoned_training_data, poisoned_training_label = backdoor_training_dataset.get_data()\n",
        "\n",
        "        backdoor_test_dataset = BackdoorDataset(\n",
        "            clean_data=testing_data,\n",
        "            clean_labels=tf.one_hot(testing_label, depth=43).numpy(),\n",
        "            trigger_obj=trigger_generator_SQRT,  # Adjust this as needed for the trigger type\n",
        "            epsilon=poison_rate,  # Apply the same poisoning rate as training\n",
        "            target_label=target_label,\n",
        "            source_label=source_label,\n",
        "            train=False,  # Specify that this is for testing\n",
        "            cifar=False\n",
        "\n",
        "        )\n",
        "        poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
        "\n",
        "        training_label = tf.reshape(tf.one_hot(training_label, depth=43, axis=1, dtype=tf.float64), (len(training_label), 43)).numpy()\n",
        "        testing_label = tf.reshape(tf.one_hot(testing_label, depth=43, axis=1, dtype=tf.float64), (len(testing_label), 43)).numpy()\n",
        "\n",
        "        indices = tf.range(start=0, limit=tf.shape(training_data)[0], dtype=tf.int32)\n",
        "        shuffled_indices = tf.random.shuffle(indices)\n",
        "        training_data = tf.gather(training_data, shuffled_indices, axis=0)\n",
        "        training_label = tf.gather(training_label, shuffled_indices, axis=0)\n",
        "        poisoned_training_data = tf.gather(poisoned_training_data, shuffled_indices, axis=0)\n",
        "        poisoned_training_label = tf.gather(poisoned_training_label, shuffled_indices, axis=0)\n",
        "\n",
        "        if isinstance(backdoor_training_dataset.trigger_obj, GenerateSQRTrigger):  # Check if the trigger is the square trigger\n",
        "            poisoned_training_data = poisoned_training_data / 255\n",
        "            poisoned_testing_data = poisoned_testing_data / 255\n",
        "\n",
        "        training_data=training_data/255\n",
        "        training_data=tf.cast(training_data,dtype=tf.dtypes.float32)\n",
        "        poisoned_training_data = tf.cast(poisoned_training_data, dtype=tf.dtypes.float32)\n",
        "        poisoned_testing_data = tf.cast(poisoned_testing_data, dtype=tf.dtypes.float32)\n",
        "\n",
        "\n",
        "        testing_data = testing_data / 255\n",
        "        testing_data = tf.cast(testing_data, dtype=tf.dtypes.float32)\n",
        "\n",
        "\n",
        "    image_patch = poisoned_testing_data[4536:4537]\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(image_patch[0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Crafted Trigger\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    for i in range(1):\n",
        "        # Creating the model\n",
        "        model_input = tf.keras.Input(shape=(poisoned_training_data.shape[1], poisoned_training_data.shape[2], poisoned_training_data.shape[3]))\n",
        "        if CIFAR10 is True:\n",
        "          model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=10)\n",
        "        else:\n",
        "          model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=43)\n",
        "\n",
        "        # Modl Aggregation\n",
        "        model = tf.keras.Model(model_input, model_output)\n",
        "        # Model Compilation\n",
        "        model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['categorical_accuracy'])\n",
        "\n",
        "        # Callbacks\n",
        "        z = []\n",
        "        weights_dict = {}\n",
        "        patch_assignments = []\n",
        "        def capture_patch_assignments(epoch, logs):\n",
        "            try:\n",
        "                intermediate_model = tf.keras.Model(\n",
        "                    inputs=model.input,\n",
        "                    outputs=[layer.output[1] for layer in model.layers if isinstance(layer, gate)]\n",
        "                )\n",
        "                # print(\"Intermediate model outputs:\", intermediate_model.outputs)\n",
        "\n",
        "                patch_indices = intermediate_model.predict(image_patch, batch_size=128)\n",
        "                patch_indices = np.array(patch_indices)\n",
        "                epoch_assignments = []  # To store assignments for this epoch\n",
        "                for expert_idx, indices in enumerate(patch_indices):\n",
        "                    # Map indices to experts\n",
        "                    flattened_indices = indices.flatten()\n",
        "                    grid_coordinates = [(i // 8, i % 8) for i in flattened_indices]  # Convert to (row, col)\n",
        "                    epoch_assignments.append({\n",
        "                        \"expert\": expert_idx + 1,\n",
        "                        \"flat_indices\": flattened_indices,\n",
        "                        \"grid_coordinates\": grid_coordinates\n",
        "                    })\n",
        "\n",
        "                patch_assignments.append(epoch_assignments)\n",
        "                print(f\"Epoch {epoch+1}: Captured patch assignments.\")\n",
        "\n",
        "                plt.figure(figsize=(3, 3))\n",
        "                plt.imshow(image_patch[0])\n",
        "                plt.title(f\"Image at Epoch {epoch+1}\")\n",
        "                plt.axis(\"off\")\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "                print(f\"Epoch {epoch+1}: Captured patch assignments.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                print(f\"Error capturing patch assignments at epoch {epoch+1}: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "\n",
        "        assignment_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=capture_patch_assignments)\n",
        "        weight_callback = tf.keras.callbacks.LambdaCallback(\n",
        "            on_epoch_end=lambda epoch, logs: weights_dict.update({epoch: model.get_weights()}))\n",
        "\n",
        "        testing_after_epoch = tf.keras.callbacks.LambdaCallback(\n",
        "            on_epoch_end=lambda epoch, logs: z.append(model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1)))\n",
        "\n",
        "        # Train the Model\n",
        "        x = model.fit(poisoned_training_data, poisoned_training_label, batch_size=128, epochs=25,\n",
        "                      callbacks=[testing_after_epoch, weight_callback, assignment_callback])\n",
        "        f = 'test_acc_loss_cifar_10_no_noise_wideresnet_moe_s_' + str(s // 1000) + 'k_v' + str(i + 1)\n",
        "        np.save(f, z)\n",
        "\n",
        "        asr = calculate_ASR(\n",
        "            model=model,\n",
        "            test_data=poisoned_testing_data,\n",
        "            test_labels=poisoned_testing_label,\n",
        "            target_label=target_label,\n",
        "            source_label=source_label,  # None if source-agnostic\n",
        "            verbose=True\n",
        "        )\n",
        "        print(f\"Attack Success Rate (ASR) for s={s}, run {i + 1}: {asr}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "ZGRgFIlNGcBx",
        "outputId": "8fc924cc-63cd-4757-c3e7-31bff86076f9"
      },
      "outputs": [],
      "source": [
        "def avg_stddev_calc(f, no_v):\n",
        "    t_1 = None\n",
        "    for i in range(no_v):\n",
        "        f_t = f + '_v' + str(i + 1) + '.npy'  # Construct the file name\n",
        "        print(f\"Looking for file: {f_t}\")  # Debugging: Print the file path\n",
        "\n",
        "        try:\n",
        "            t = np.load(f_t)  # Attempt to load the file\n",
        "            print(f\"Loaded file: {f_t}\")  # Confirm the file was loaded successfully\n",
        "            t = tf.reshape(t, (1, tf.shape(t)[0], tf.shape(t)[1])).numpy()\n",
        "\n",
        "            if t_1 is None:\n",
        "                t_1 = t\n",
        "            else:\n",
        "                t_1 = tf.concat((t_1, t), axis=0).numpy()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File {f_t} not found. Skipping.\")\n",
        "\n",
        "    if t_1 is None:\n",
        "        raise ValueError(\"No valid files found for computation.\")\n",
        "\n",
        "    t_av = tf.math.reduce_mean(t_1, axis=0).numpy()\n",
        "    t_std = tf.math.reduce_std(t_1, axis=0).numpy()\n",
        "    return t_av, t_std\n",
        "\n",
        "\n",
        "def last_epoch_result_collection(f, no_sample_points, points, no_v=5, last_epoch=50):\n",
        "    t_av_s = np.zeros((no_sample_points, 3), dtype=np.float64)\n",
        "    t_std_s = np.zeros((no_sample_points, 3), dtype=np.float64)\n",
        "\n",
        "    for i in range(no_sample_points):\n",
        "        f_1 = f + '_s_' + str(points[i] // 1000) + 'k'  # Construct base filename for each sample point\n",
        "        print(f\"Processing sample point {points[i]}: {f_1}\")  # Debugging: Print base file name\n",
        "\n",
        "        t_av, t_std = avg_stddev_calc(f_1, no_v)  # Get average and stddev\n",
        "\n",
        "        t_av_s[i, 0] = t_av[last_epoch - 1, 0]  # Accuracy at the last epoch\n",
        "        t_av_s[i, 1] = t_av[last_epoch - 1, 1]  # Loss at the last epoch\n",
        "        t_av_s[i, 2] = points[i]  # Sample size\n",
        "\n",
        "        t_std_s[i, 0] = t_std[last_epoch - 1, 0]  # Stddev of accuracy\n",
        "        t_std_s[i, 1] = t_std[last_epoch - 1, 1]  # Stddev of loss\n",
        "        t_std_s[i, 2] = points[i]  # Sample size\n",
        "\n",
        "    return t_av_s, t_std_s\n",
        "\n",
        "# Set file and sample information\n",
        "f_moe = 'test_acc_loss_cifar_10_no_noise_wideresnet_moe'\n",
        "no_sample_points = 1\n",
        "points = [50000]\n",
        "no_v = 1\n",
        "last_epoch = 25\n",
        "\n",
        "# Perform analysis\n",
        "try:\n",
        "    wideresnet_moe_av_s, wideresnet_moe_std_s = last_epoch_result_collection(f_moe, no_sample_points, points, no_v, last_epoch)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Average Accuracy and Loss (last epoch):\")\n",
        "    print(wideresnet_moe_av_s)\n",
        "    print(\"Standard Deviation (last epoch):\")\n",
        "    print(wideresnet_moe_std_s)\n",
        "\n",
        "    # Plot results\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.errorbar(wideresnet_moe_av_s[:, 2], wideresnet_moe_av_s[:, 1], wideresnet_moe_std_s[:, 1],\n",
        "                 marker='^', label='Wideresnet-MoE')\n",
        "    plt.legend()\n",
        "    plt.xlabel('No. of training samples')\n",
        "    plt.ylabel('Test accuracy')\n",
        "    plt.title('CIFAR-10: Wideresnet vs. Wideresnet-MoE')\n",
        "    plt.show()\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Error during analysis: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "cO7PdUsAGcBx",
        "outputId": "c22a711e-2fdc-4c5a-8bdb-26900c4b6d52"
      },
      "outputs": [],
      "source": [
        "grid_tracking = {expert_id: np.zeros((8, 8), dtype=int) for expert_id in range(1, 5)}\n",
        "for epoch_assignments in patch_assignments:\n",
        "    for assignment in epoch_assignments:\n",
        "        expert_id = assignment[\"expert\"]\n",
        "        for (row, col) in assignment[\"grid_coordinates\"]:\n",
        "            grid_tracking[expert_id][row, col] += 1\n",
        "visualize_expert_specialization(grid_tracking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HFXtOfPj7Ug0",
        "outputId": "5837cf4f-6837-4e40-c83e-cbea0c997a78"
      },
      "outputs": [],
      "source": [
        "visualize_expert_specialization_by_epoch(patch_assignments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Patch value distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P0OVzmh61zwE",
        "outputId": "410939b7-95d8-4662-f223-04761989cdd8"
      },
      "outputs": [],
      "source": [
        "# Compute patch sums for the training data\n",
        "patch_sums = compute_patch_sums(image_patch)\n",
        "\n",
        "expert_patch_values = {expert_id: [] for expert_id in range(1, 5)}\n",
        "\n",
        "for epoch_assignments, image_patch_sums in zip(patch_assignments, patch_sums):\n",
        "    for assignment, image_patch_sum in zip(epoch_assignments, image_patch_sums):\n",
        "        expert_id = assignment[\"expert\"]\n",
        "        for row, col in assignment[\"grid_coordinates\"]:\n",
        "            # Safely access the value for the given (row, col)\n",
        "            value = image_patch_sum[row, col] if len(image_patch_sum.shape) > 1 else image_patch_sum\n",
        "            expert_patch_values[expert_id].append(value)\n",
        "\n",
        "\n",
        "last_epoch_data = patch_assignments[-1]\n",
        "expert_patch_assignments = {}\n",
        "for expert_data in last_epoch_data:\n",
        "    expert = expert_data['expert']\n",
        "    grid_coordinates = expert_data['grid_coordinates']\n",
        "\n",
        "    # Create a list to store the patch sums for this expert\n",
        "    expert_patch_values = []\n",
        "\n",
        "    # Iterate over grid coordinates and fetch patch sum values\n",
        "    for (x, y) in grid_coordinates:\n",
        "        patch_value = patch_sums[0, x, y]  # Get the patch sum value from patch_sums array\n",
        "        expert_patch_values.append(patch_value)\n",
        "\n",
        "    # Assign the expert's patch values\n",
        "    expert_patch_assignments[expert] = expert_patch_values\n",
        "\n",
        "# Display the expert patch assignments for the last epoch\n",
        "print(patch_assignments[-1])\n",
        "print(patch_sums)\n",
        "np.save('patch sums', patch_sums)\n",
        "for expert, patches in expert_patch_assignments.items():\n",
        "    print(f\"Expert {expert}:\")\n",
        "    print(f\"Assigned Patch Values: {patches}\\n\")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot histogram for each expert\n",
        "for i, expert_data in enumerate(last_epoch_data):\n",
        "    expert = expert_data['expert']\n",
        "    grid_coordinates = expert_data['grid_coordinates']\n",
        "\n",
        "    # Extract patch values for this expert based on the grid coordinates\n",
        "    patch_values = [patch_sums[0, x, y] for (x, y) in grid_coordinates]\n",
        "\n",
        "    # Plot histogram for the expert\n",
        "    plt.subplot(2, 2, i + 1)  # Create subplots, adjusting as needed\n",
        "    plt.hist(patch_values, bins=10, color='skyblue', edgecolor='black')\n",
        "    plt.title(f'Patch Value Distribution for Expert {expert}')\n",
        "    plt.xlabel('Patch Value')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pruning for defense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvl7-udxILP5"
      },
      "outputs": [],
      "source": [
        "class KerasPruning:\n",
        "    def __init__(self, x_train, y_train, model, layer_names, prune_rate):\n",
        "        \"\"\"\n",
        "        Prunes specific layers in a Keras model.\n",
        "\n",
        "        Args:\n",
        "            model (tf.keras.Model): The trained Keras model.\n",
        "            layer_names (list): List of names of layers to prune.\n",
        "            prune_rate (float): Fraction of filters to remove.\n",
        "            x_train (tf.data.Dataset): Dataset for computing activations.\n",
        "            y_train (tf.data.Dataset): Corresponding labels for dataset.\n",
        "        \"\"\"\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.model = model\n",
        "        self.layer_names = layer_names  # List of layer names to prune\n",
        "        self.prune_rate = prune_rate\n",
        "\n",
        "    def get_layer_activations(self, layer_name):\n",
        "        \"\"\"\n",
        "        Runs the dataset through the model and collects activations of the target layer.\n",
        "        \"\"\"\n",
        "        activation_model = tf.keras.Model(\n",
        "            inputs=self.model.input,\n",
        "            outputs=self.model.get_layer(layer_name).output\n",
        "        )\n",
        "\n",
        "        batch_size = 8\n",
        "        activations = []\n",
        "        for i in range(0, len(self.x_train), batch_size):\n",
        "            batch = self.x_train[i:i+batch_size]\n",
        "            batch_activations = activation_model(batch, training=False)\n",
        "            activations.append(batch_activations)\n",
        "\n",
        "        return tf.concat(activations, axis=0)  # Shape: (num_samples, H, W, C)\n",
        "\n",
        "    def prune(self):\n",
        "        \"\"\"\n",
        "        Prunes filters in the selected layers based on their average activation.\n",
        "        \"\"\"\n",
        "        for layer_name in self.layer_names:\n",
        "            print(f\"Pruning layer: {layer_name}\")\n",
        "\n",
        "            # Get the layer\n",
        "            layer = self.model.get_layer(layer_name)\n",
        "\n",
        "            # Get activations for the layer\n",
        "            activations = self.get_layer_activations(layer_name)\n",
        "            mean_activations = tf.reduce_mean(activations, axis=[0, 1, 2])  # Shape: (C,)\n",
        "\n",
        "            # Sort filters by activation\n",
        "            num_filters = mean_activations.shape[0]\n",
        "            num_pruned_filters = int(num_filters * self.prune_rate)\n",
        "            sorted_indices = tf.argsort(mean_activations)[:num_pruned_filters]  # Least active filters\n",
        "\n",
        "            # Get layer weights\n",
        "            weights, biases = layer.get_weights()  # Weights shape: (H, W, C_in, C_out)\n",
        "\n",
        "            # Set pruned filters to zero\n",
        "            weights[:, :, :, sorted_indices.numpy()] = 0\n",
        "            biases[sorted_indices.numpy()] = 0\n",
        "\n",
        "            # Assign updated weights back to the layer\n",
        "            layer.set_weights([weights, biases])\n",
        "\n",
        "            print(f\"Pruned {num_pruned_filters}/{num_filters} filters in {layer_name}\")\n",
        "\n",
        "        return self.model  # Return pruned model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85D_7PJoRAhC"
      },
      "outputs": [],
      "source": [
        "def get_layer_by_index(model, layer_type, index):\n",
        "    \"\"\"\n",
        "    Retrieves the correct layer name dynamically based on its type and order.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): The trained model.\n",
        "        layer_type (tf.keras.layers.Layer): The type of layer to search for (e.g., tf.keras.layers.Conv2D).\n",
        "        index (int): The occurrence index of the layer (0-based).\n",
        "\n",
        "    Returns:\n",
        "        str: The dynamically assigned name of the layer.\n",
        "    \"\"\"\n",
        "    layers = [layer.name for layer in model.layers if isinstance(layer, layer_type)]\n",
        "\n",
        "    if index >= len(layers):\n",
        "        raise ValueError(f\"Model has only {len(layers)} layers of type {layer_type}, but index {index} was requested.\")\n",
        "\n",
        "    return layers[index]  # Return the dynamic layer name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Layers to be pruned, these are the last convolutional layers for each expert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnd49BQiRDzU",
        "outputId": "447a1fe1-bc98-4a3e-dda8-edeb4a2c225d"
      },
      "outputs": [],
      "source": [
        "layer_names_to_prune = [\n",
        "    get_layer_by_index(model, tf.keras.layers.Conv2D, 8),  \n",
        "    get_layer_by_index(model, tf.keras.layers.Conv2D, 9),  \n",
        "    get_layer_by_index(model, tf.keras.layers.Conv2D, 10),   \n",
        "    get_layer_by_index(model, tf.keras.layers.Conv2D, 11),\n",
        "]\n",
        "\n",
        "print(f\"Selected layers to prune: {layer_names_to_prune}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pruning the backdoor model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNfcQKgIMn5"
      },
      "outputs": [],
      "source": [
        "pruner = KerasPruning(x_train=training_data[:10000], y_train=training_label[:10000], model=model, layer_names=layer_names_to_prune, prune_rate=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_qH3bawPLPx",
        "outputId": "d5ef4485-8120-494c-a104-fc41d1757d89"
      },
      "outputs": [],
      "source": [
        "pruned_model = pruner.prune()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKjbyr63Hw-y",
        "outputId": "8af39998-59bb-49c2-fb32-f8defc6b65b5"
      },
      "outputs": [],
      "source": [
        "attack_acc = calculate_ASR(\n",
        "            model=pruned_model,\n",
        "            test_data=poisoned_testing_data,\n",
        "            test_labels=poisoned_testing_label,\n",
        "            target_label=target_label,\n",
        "            source_label=source_label,  # None if source-agnostic\n",
        "            verbose=True\n",
        "        )\n",
        "print(f\"Attack Success Rate (ASR) for s={s}, run {i + 1}: {attack_acc}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get test accuracy after pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVOwAKzMIOOC",
        "outputId": "930f977d-d2be-4f44-be10-5de9222492fc"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = pruned_model.evaluate(testing_data, testing_label, batch_size=1000)\n",
        "print(test_loss, test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rLO9zbBYUOMQ",
        "outputId": "c3f5dfa5-9fff-41e0-d4b3-73a05214bfe8"
      },
      "outputs": [],
      "source": [
        "x = pruned_model.fit(training_data,training_label,batch_size=128,epochs=5,callbacks=[testing_after_epoch, weight_callback, assignment_callback])\n",
        "f='test_acc_loss_cifar_10_no_noise_wideresnet_moe_s_'+str(s//1000)+'k_v'+str(i+1)\n",
        "np.save(f,z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "G2D5RluXWPu1",
        "outputId": "05ef4c78-bce2-4a7e-8764-a2ad2956ba2c"
      },
      "outputs": [],
      "source": [
        "grid_tracking = {expert_id: np.zeros((8, 8), dtype=int) for expert_id in range(1, 5)}\n",
        "for epoch_assignments in patch_assignments:\n",
        "    for assignment in epoch_assignments:\n",
        "        expert_id = assignment[\"expert\"]\n",
        "        for (row, col) in assignment[\"grid_coordinates\"]:\n",
        "            grid_tracking[expert_id][row, col] += 1\n",
        "visualize_expert_specialization(grid_tracking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vSgQTEr5Wz1x",
        "outputId": "13c0b1ea-0669-4a73-e9c1-a554a4296c64"
      },
      "outputs": [],
      "source": [
        "visualize_expert_specialization_by_epoch(patch_assignments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute ASR after pruning and fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT93kJ42U-g0",
        "outputId": "f598d6a3-1a28-418c-a1bb-d77c53e14d9d"
      },
      "outputs": [],
      "source": [
        "attack_acc = calculate_ASR(\n",
        "            model=pruned_model,\n",
        "            test_data=poisoned_testing_data,\n",
        "            test_labels=poisoned_testing_label,\n",
        "            target_label=target_label,\n",
        "            source_label=source_label,  # None if source-agnostic\n",
        "            verbose=True\n",
        "        )\n",
        "print(f\"Attack Success Rate (ASR) for s={s}, run {i + 1}: {attack_acc}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXPSSmCEWauA",
        "outputId": "ec8acb4e-2ec6-4fcc-fe86-c57108a7491e"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = pruned_model.evaluate(testing_data, testing_label, batch_size=1000)\n",
        "print(test_loss, test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
