{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from random import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import traceback\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gating router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gate(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, gating_kernel_size, strides=(1,1), padding = 'valid',\n",
    "                 data_format = 'channels_last', gating_activation = None,\n",
    "                 gating_kernel_initializer = tf.keras.initializers.RandomNormal, **kwargs):\n",
    "\n",
    "        super(gate, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "        self.gating_kernel_size = gating_kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.gating_activation = tf.keras.activations.get(gating_activation)\n",
    "        self.gating_kernel_initializer = gating_kernel_initializer\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
    "\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        gating_kernel_shape = self.gating_kernel_size + (input_dim, 1)\n",
    "        self.gating_kernel = self.add_weight(shape=gating_kernel_shape,\n",
    "                                      initializer=self.gating_kernel_initializer,\n",
    "                                      name='gating_kernel')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        gating_outputs = tf.keras.backend.conv2d(inputs, self.gating_kernel, strides=self.strides,\n",
    "                                  padding=self.padding,data_format=self.data_format)\n",
    "\n",
    "        gating_outputs = tf.transpose(gating_outputs, perm=(0,3,1,2))\n",
    "        x = tf.shape(gating_outputs)[2]\n",
    "        y = tf.shape(gating_outputs)[3]\n",
    "        gating_outputs = tf.reshape(gating_outputs,(tf.shape(gating_outputs)[0],tf.shape(gating_outputs)[1],\n",
    "                                                    x*y))\n",
    "\n",
    "        gating_outputs = self.gating_activation(gating_outputs)\n",
    "        # print(\"gating output: \", gating_outputs.shape)\n",
    "        [values, indices] = tf.math.top_k(gating_outputs,k=self.k, sorted=False)\n",
    "        # print(\"value output: \", values.shape)\n",
    "        # print(\"indice before output: \", indices.shape)\n",
    "        indices = tf.reshape(indices,(tf.shape(indices)[0]*tf.shape(indices)[1],tf.shape(indices)[2]))\n",
    "        # print(\"indice after output: \", indices.shape)\n",
    "        values = tf.reshape(values, (tf.shape(values)[0]*tf.shape(values)[1], tf.shape(values)[2]))\n",
    "        batch_t, k_t = tf.unstack(tf.shape(indices), num=2)\n",
    "\n",
    "        n=tf.shape(gating_outputs)[2]\n",
    "\n",
    "        indices_flat = tf.reshape(indices, [-1]) + tf.math.floordiv(tf.range(batch_t * k_t), k_t) * n\n",
    "        ret_flat = tf.math.unsorted_segment_sum(tf.reshape(values, [-1]), indices_flat, batch_t * n)\n",
    "        ret_rsh=tf.reshape(ret_flat, [batch_t, n])\n",
    "        ret_rsh_3=tf.reshape(ret_rsh,(tf.shape(gating_outputs)[0],tf.shape(gating_outputs)[1],tf.shape(gating_outputs)[2]))\n",
    "\n",
    "        new_gating_outputs = tf.reshape(ret_rsh_3,(tf.shape(ret_rsh_3)[0],tf.shape(ret_rsh_3)[1],x,y))\n",
    "        new_gating_outputs = tf.transpose(new_gating_outputs, perm=(0,2,3,1))\n",
    "        new_gating_outputs = tf.repeat(new_gating_outputs,tf.shape(self.gating_kernel)[0]*tf.shape(self.gating_kernel)[1]*tf.shape(self.gating_kernel)[2],axis=3)\n",
    "        new_gating_outputs=tf.reshape(new_gating_outputs,(tf.shape(new_gating_outputs)[0],tf.shape(new_gating_outputs)[1],tf.shape(new_gating_outputs)[2],tf.shape(self.gating_kernel)[0],tf.shape(self.gating_kernel)[1],tf.shape(self.gating_kernel)[2]))\n",
    "        new_gating_outputs=tf.transpose(new_gating_outputs,perm=(0,1,3,2,4,5))\n",
    "        new_gating_outputs=tf.reshape(new_gating_outputs,(tf.shape(new_gating_outputs)[0],tf.shape(new_gating_outputs)[1]*tf.shape(new_gating_outputs)[2],tf.shape(new_gating_outputs)[3]*tf.shape(new_gating_outputs)[4],tf.shape(new_gating_outputs)[5]))\n",
    "        outputs = inputs*new_gating_outputs\n",
    "        return outputs, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer_gate=keras.initializers.RandomNormal(mean=0.0,stddev=0.0001)\n",
    "\n",
    "def WideResnetBlock(x, channels, strides, channel_mismatch=False):\n",
    "\n",
    "    identity = x\n",
    "\n",
    "    out = layers.BatchNormalization()(x)\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=strides, padding='same')(out)\n",
    "\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=1, padding='same')(out)\n",
    "\n",
    "    if channel_mismatch is not False:\n",
    "        identity = layers.Conv2D(filters=channels, kernel_size=1, strides=strides, padding='valid')(identity)\n",
    "\n",
    "    out = layers.Add()([identity, out])\n",
    "\n",
    "    return out\n",
    "\n",
    "def WideResnetGroup(x, num_blocks, channels, strides):\n",
    "\n",
    "    x = WideResnetBlock(x=x, channels=channels, strides=strides, channel_mismatch=True)\n",
    "\n",
    "    for _ in range(num_blocks - 1):\n",
    "        x = WideResnetBlock(x=x, channels=channels, strides=(1, 1))\n",
    "\n",
    "    return x\n",
    "\n",
    "def WideResnet(x, num_blocks, k, num_classes=10):\n",
    "    widths = [int(v * k) for v in (16, 32, 64)]\n",
    "\n",
    "    x = layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = WideResnetGroup(x, num_blocks, widths[0], strides=(1, 1))\n",
    "    x = WideResnetGroup(x, num_blocks, widths[1], strides=(2, 2))\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=640, kernel_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    x_1, indices_1 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_2, indices_2 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_3, indices_3 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_4, indices_4 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
    "\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_3 = layers.BatchNormalization()(x_3)\n",
    "    x_4 = layers.BatchNormalization()(x_4)\n",
    "\n",
    "    x_1 = layers.ReLU()(x_1)\n",
    "    x_2 = layers.ReLU()(x_2)\n",
    "    x_3 = layers.ReLU()(x_3)\n",
    "    x_4 = layers.ReLU()(x_4)\n",
    "\n",
    "    x_1 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_1)\n",
    "    x_2 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_2)\n",
    "    x_3 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_3)\n",
    "    x_4 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_4)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([x_1, x_2, x_3, x_4])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.AveragePooling2D((8,8))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=num_classes, activation='softmax')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the correct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10 = True #If false, you use the GTSRB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [50000]:\n",
    "    if CIFAR10 is True:\n",
    "      print(\"CIFAR-10 as dataset\")\n",
    "      #Loading the Data\n",
    "      training_data_all = np.load('cifar_10_train_data_sorted.npy')\n",
    "      training_label_all = np.load('cifar_10_train_label_sorted.npy')\n",
    "      testing_data = np.load('cifar_10_test_data_sorted.npy')\n",
    "      testing_label = np.load('cifar_10_test_label_sorted.npy')\n",
    "\n",
    "      #sampling training data\n",
    "      training_data=np.concatenate((training_data_all[0:0+(s//10)],training_data_all[5000:5000+(s//10)],training_data_all[10000:10000+(s//10)],training_data_all[15000:15000+(s//10)],training_data_all[20000:20000+(s//10)],training_data_all[25000:25000+(s//10)],training_data_all[30000:30000+(s//10)],training_data_all[35000:35000+(s//10)],training_data_all[40000:40000+(s//10)],training_data_all[45000:45000+(s//10)]),axis=0)\n",
    "      training_label=np.concatenate((training_label_all[0:0+(s//10)],training_label_all[5000:5000+(s//10)],training_label_all[10000:10000+(s//10)],training_label_all[15000:15000+(s//10)],training_label_all[20000:20000+(s//10)],training_label_all[25000:25000+(s//10)],training_label_all[30000:30000+(s//10)],training_label_all[35000:35000+(s//10)],training_label_all[40000:40000+(s//10)],training_label_all[45000:45000+(s//10)]),axis=0)\n",
    "\n",
    "      # 1-of-K encoding\n",
    "      training_label = tf.reshape(tf.one_hot(training_label, axis=1, depth=10,dtype=tf.float64),(s,10)).numpy()\n",
    "      testing_label = tf.reshape(tf.one_hot(testing_label, axis=1, depth=10, dtype=tf.float64),(10000,10)).numpy()\n",
    "\n",
    "      #shuffling the training set\n",
    "      indices = tf.range(start=0, limit=tf.shape(training_data)[0], dtype=tf.int32)\n",
    "      shuffled_indices = tf.random.shuffle(indices)\n",
    "      training_data = tf.gather(training_data, shuffled_indices, axis=0)\n",
    "      training_label = tf.gather(training_label, shuffled_indices, axis=0)\n",
    "\n",
    "      #normalizing and reshaping data\n",
    "      training_data=training_data/255\n",
    "      training_data=tf.cast(training_data,dtype=tf.dtypes.float32)\n",
    "      testing_data=testing_data/255\n",
    "      testing_data=tf.cast(testing_data,dtype=tf.dtypes.float32)\n",
    "\n",
    "    else:\n",
    "      print(\"GTSRB as dataset\")\n",
    "      training_data = np.load('gtsrb_train_data_sorted.npy')\n",
    "      training_label = np.load('gtsrb_train_label_sorted.npy')\n",
    "      testing_data = np.load('gtsrb_test_data_sorted.npy')\n",
    "      testing_label = np.load('gtsrb_test_label_sorted.npy')\n",
    "\n",
    "      training_label = tf.reshape(tf.one_hot(training_label, depth=43, axis=1, dtype=tf.float64), (len(training_label), 43)).numpy()\n",
    "      testing_label = tf.reshape(tf.one_hot(testing_label, depth=43, axis=1, dtype=tf.float64), (len(testing_label), 43)).numpy()\n",
    "\n",
    "      indices = tf.range(start=0, limit=tf.shape(training_data)[0], dtype=tf.int32)\n",
    "      shuffled_indices = tf.random.shuffle(indices)\n",
    "      training_data = tf.gather(training_data, shuffled_indices, axis=0)\n",
    "      training_label = tf.gather(training_label, shuffled_indices, axis=0)\n",
    "\n",
    "      training_data=training_data/255\n",
    "      training_data=tf.cast(training_data,dtype=tf.dtypes.float32)\n",
    "      testing_data=testing_data/255\n",
    "      testing_data=tf.cast(testing_data,dtype=tf.dtypes.float32)\n",
    "\n",
    "\n",
    "    image_patch = testing_data[1531:1532]\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image_patch[0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Crafted Trigger\")\n",
    "    plt.show()\n",
    "    for i in range(1):\n",
    "        #Creating the model\n",
    "        model_input = tf.keras.Input(shape=( training_data.shape[1], training_data.shape[2], training_data.shape[3]))\n",
    "        if CIFAR10 is True:\n",
    "          model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=10)\n",
    "        else:\n",
    "          model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=43)\n",
    "\n",
    "        #Model Aggregation\n",
    "        model=tf.keras.Model(model_input,model_output)\n",
    "        #Model Compilation\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),loss='categorical_crossentropy',\n",
    "                      metrics=['categorical_accuracy'])\n",
    "\n",
    "        #Call backs\n",
    "        z=[]\n",
    "        weights_dict = {}\n",
    "        patch_assignments = []\n",
    "        def capture_patch_assignments(epoch, logs):\n",
    "            try:\n",
    "                intermediate_model = tf.keras.Model(\n",
    "                    inputs=model.input,\n",
    "                    outputs=[layer.output[1] for layer in model.layers if isinstance(layer, gate)]\n",
    "                )\n",
    "                print(\"Intermediate model outputs:\", intermediate_model.outputs)\n",
    "\n",
    "                patch_indices = intermediate_model.predict(image_patch, batch_size=128)\n",
    "                patch_indices = np.array(patch_indices)\n",
    "                epoch_assignments = []  # To store assignments for this epoch\n",
    "                correlation_data = []\n",
    "                for expert_idx, indices in enumerate(patch_indices):\n",
    "                    # Map indices to experts\n",
    "                    flattened_indices = indices.flatten()\n",
    "                    grid_coordinates = [(i // 8, i % 8) for i in flattened_indices]  # Convert to (row, col)\n",
    "                    epoch_assignments.append({\n",
    "                        \"expert\": expert_idx + 1,\n",
    "                        \"flat_indices\": flattened_indices,\n",
    "                        \"grid_coordinates\": grid_coordinates\n",
    "                    })\n",
    "\n",
    "                patch_assignments.append(epoch_assignments)\n",
    "                print(f\"Epoch {epoch+1}: Captured patch assignments.\")\n",
    "\n",
    "\n",
    "                plt.figure(figsize=(3, 3))\n",
    "                plt.imshow(image_patch[0])\n",
    "                plt.title(f\"Image at Epoch {epoch+1}\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                print(f\"Epoch {epoch+1}: Captured patch assignments.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error capturing patch assignments at epoch {epoch}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        assignment_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=capture_patch_assignments)\n",
    "        weight_callback = tf.keras.callbacks.LambdaCallback \\\n",
    "                                          ( on_epoch_end=lambda epoch, logs: weights_dict.update({epoch:model.get_weights()}))\n",
    "\n",
    "\n",
    "        testing_after_epoch = tf.keras.callbacks.LambdaCallback(on_epoch_end = lambda epoch, logs: z.append(model.evaluate(testing_data, testing_label, batch_size=1000,verbose=1)))\n",
    "\n",
    "        #Train the Model\n",
    "        x=model.fit(training_data,training_label,batch_size=128,epochs=25,callbacks=[testing_after_epoch, weight_callback, assignment_callback])\n",
    "        f='test_acc_loss_cifar_10_no_noise_wideresnet_moe_s_'+str(s//1000)+'k_v'+str(i+1)\n",
    "        np.save(f,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_stddev_calc(f, no_v):\n",
    "    t_1 = None\n",
    "    for i in range(no_v):\n",
    "        f_t = f + '_v' + str(i + 1) + '.npy'  # Construct the file name\n",
    "        print(f\"Looking for file: {f_t}\")  # Debugging: Print the file path\n",
    "\n",
    "        try:\n",
    "            t = np.load(f_t)  # Attempt to load the file\n",
    "            print(f\"Loaded file: {f_t}\")  # Confirm the file was loaded successfully\n",
    "            t = tf.reshape(t, (1, tf.shape(t)[0], tf.shape(t)[1])).numpy()\n",
    "\n",
    "            if t_1 is None:\n",
    "                t_1 = t\n",
    "            else:\n",
    "                t_1 = tf.concat((t_1, t), axis=0).numpy()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {f_t} not found. Skipping.\")\n",
    "\n",
    "    if t_1 is None:\n",
    "        raise ValueError(\"No valid files found for computation.\")\n",
    "\n",
    "    t_av = tf.math.reduce_mean(t_1, axis=0).numpy()\n",
    "    t_std = tf.math.reduce_std(t_1, axis=0).numpy()\n",
    "    return t_av, t_std\n",
    "\n",
    "\n",
    "def last_epoch_result_collection(f, no_sample_points, points, no_v=5, last_epoch=50):\n",
    "    t_av_s = np.zeros((no_sample_points, 3), dtype=np.float64)\n",
    "    t_std_s = np.zeros((no_sample_points, 3), dtype=np.float64)\n",
    "\n",
    "    for i in range(no_sample_points):\n",
    "        f_1 = f + '_s_' + str(points[i] // 1000) + 'k'  # Construct base filename for each sample point\n",
    "        print(f\"Processing sample point {points[i]}: {f_1}\")  # Debugging: Print base file name\n",
    "\n",
    "        t_av, t_std = avg_stddev_calc(f_1, no_v)  # Get average and stddev\n",
    "\n",
    "        t_av_s[i, 0] = t_av[last_epoch - 1, 0]  # Accuracy at the last epoch\n",
    "        t_av_s[i, 1] = t_av[last_epoch - 1, 1]  # Loss at the last epoch\n",
    "        t_av_s[i, 2] = points[i]  # Sample size\n",
    "\n",
    "        t_std_s[i, 0] = t_std[last_epoch - 1, 0]  # Stddev of accuracy\n",
    "        t_std_s[i, 1] = t_std[last_epoch - 1, 1]  # Stddev of loss\n",
    "        t_std_s[i, 2] = points[i]  # Sample size\n",
    "\n",
    "    return t_av_s, t_std_s\n",
    "\n",
    "# Set file and sample information\n",
    "f_moe = 'test_acc_loss_cifar_10_no_noise_wideresnet_moe'\n",
    "no_sample_points = 1\n",
    "points = [50000]\n",
    "no_v = 1\n",
    "last_epoch = 25\n",
    "\n",
    "# Perform analysis\n",
    "try:\n",
    "    wideresnet_moe_av_s, wideresnet_moe_std_s = last_epoch_result_collection(f_moe, no_sample_points, points, no_v, last_epoch)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Average Accuracy and Loss (last epoch):\")\n",
    "    print(wideresnet_moe_av_s)\n",
    "    print(\"Standard Deviation (last epoch):\")\n",
    "    print(wideresnet_moe_std_s)\n",
    "\n",
    "    # Plot results\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.errorbar(wideresnet_moe_av_s[:, 2], wideresnet_moe_av_s[:, 1], wideresnet_moe_std_s[:, 1],\n",
    "                 marker='^', label='Wideresnet-MoE')\n",
    "    plt.legend()\n",
    "    plt.xlabel('No. of training samples')\n",
    "    plt.ylabel('Test accuracy')\n",
    "    plt.title('CIFAR-10: Wideresnet vs. Wideresnet-MoE')\n",
    "    plt.show()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Error during analysis: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tracking = {expert_id: np.zeros((8, 8), dtype=int) for expert_id in range(1, 5)}\n",
    "for epoch_assignments in patch_assignments:\n",
    "    for assignment in epoch_assignments:\n",
    "        expert_id = assignment[\"expert\"]\n",
    "        for (row, col) in assignment[\"grid_coordinates\"]:\n",
    "            grid_tracking[expert_id][row, col] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of patch routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_expert_specialization(grid_tracking):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "    for expert_id, grid in grid_tracking.items():\n",
    "        ax = axes[expert_id - 1]\n",
    "        cax = ax.imshow(grid, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "        ax.set_title(f\"Expert {expert_id} Specialization\")\n",
    "        ax.set_xlabel(\"Columns\")\n",
    "        ax.set_ylabel(\"Rows\")\n",
    "        fig.colorbar(cax, ax=ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_expert_specialization(grid_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_patch_sums(images, patch_size=(8, 8)):\n",
    "    num_images, img_height, img_width, _ = images.shape\n",
    "    patch_sums = np.zeros((num_images, patch_size[0], patch_size[1]))\n",
    "\n",
    "    for i in range(patch_size[0]):  # Divide into rows\n",
    "        for j in range(patch_size[1]):  # Divide into columns\n",
    "            patch_sums[:, i, j] = np.sum(\n",
    "                images[:,\n",
    "                       i * (img_height // patch_size[0]): (i + 1) * (img_height // patch_size[0]),\n",
    "                       j * (img_width // patch_size[1]): (j + 1) * (img_width // patch_size[1]),\n",
    "                       :],\n",
    "                axis=(1, 2, 3)\n",
    "            )\n",
    "    return patch_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute patch sums for the training data\n",
    "patch_sums = compute_patch_sums(image_patch)\n",
    "\n",
    "expert_patch_values = {expert_id: [] for expert_id in range(1, 5)}\n",
    "\n",
    "for epoch_assignments, image_patch_sums in zip(patch_assignments, patch_sums):\n",
    "    for assignment, image_patch_sum in zip(epoch_assignments, image_patch_sums):\n",
    "        expert_id = assignment[\"expert\"]\n",
    "        for row, col in assignment[\"grid_coordinates\"]:\n",
    "            # Safely access the value for the given (row, col)\n",
    "            value = image_patch_sum[row, col] if len(image_patch_sum.shape) > 1 else image_patch_sum\n",
    "            expert_patch_values[expert_id].append(value)\n",
    "\n",
    "\n",
    "last_epoch_data = patch_assignments[-1]\n",
    "expert_patch_assignments = {}\n",
    "for expert_data in last_epoch_data:\n",
    "    expert = expert_data['expert']\n",
    "    grid_coordinates = expert_data['grid_coordinates']\n",
    "\n",
    "    # Create a list to store the patch sums for this expert\n",
    "    expert_patch_values = []\n",
    "\n",
    "    # Iterate over grid coordinates and fetch patch sum values\n",
    "    for (x, y) in grid_coordinates:\n",
    "        patch_value = patch_sums[0, x, y]  # Get the patch sum value from patch_sums array\n",
    "        expert_patch_values.append(patch_value)\n",
    "\n",
    "    # Assign the expert's patch values\n",
    "    expert_patch_assignments[expert] = expert_patch_values\n",
    "\n",
    "# Display the expert patch assignments for the last epoch\n",
    "print(patch_assignments[-1])\n",
    "print(patch_sums)\n",
    "for expert, patches in expert_patch_assignments.items():\n",
    "    print(f\"Expert {expert}:\")\n",
    "    print(f\"Assigned Patch Values: {patches}\\n\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot histogram for each expert\n",
    "for i, expert_data in enumerate(last_epoch_data):\n",
    "    expert = expert_data['expert']\n",
    "    grid_coordinates = expert_data['grid_coordinates']\n",
    "\n",
    "    # Extract patch values for this expert based on the grid coordinates\n",
    "    patch_values = [patch_sums[0, x, y] for (x, y) in grid_coordinates]\n",
    "\n",
    "    # Plot histogram for the expert\n",
    "    plt.subplot(2, 2, i + 1)  # Create subplots, adjusting as needed\n",
    "    plt.hist(patch_values, bins=10, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Patch Value Distribution for Expert {expert}')\n",
    "    plt.xlabel('Patch Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
